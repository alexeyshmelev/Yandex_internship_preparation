\documentclass[a4paper,10pt, openany]{book} % openany - глава с новой страницы
\usepackage[russian]{babel} % добавление русского языка
\usepackage{amsfonts} % обозначение множеств
\usepackage{amsmath} % жирный шрифт в формулах
\pagestyle{myheadings}
\special{papersize=170mm,240mm}
\textheight 187mm % 200-(12+25)*0.35146 = 186.99598
\textwidth 130mm % максимальная ширина строчки 
\headheight13.6pt % = 0.48 mm
\oddsidemargin -5.4mm % левое поле (с учетом одного дюйма)
\topmargin -5.4mm % верхнее поле (с учетом одного дюйма)
\usepackage{setspace}
\singlespacing

% Работа с колонтитулами
\usepackage{fancybox,fancyhdr}
\pagestyle{fancy}
\fancyhead{}
\fancyhead[LE,RO]{\thepage}
\fancyhead[RE, LO]{Учебник по машинному обучению} 
\fancyfoot{}

% настройки абзацев
\usepackage{indentfirst}
\setlength{\parindent}{5ex}
\setlength{\parskip}{1em}

\begin{document}
	
	\tableofcontents % оглавление
	
	\newpage
	
	\chapter{Введение}
	
	\section{Машинное обучение}
	
	\textbf{Машинное обучение} -- это наука, изучающая алгоритмы, автоматически улучшающиеся благодаря опыту.
	
	Большинство решений задач можно представить в виде функции:
	\begin{equation*}
		\textit{Примеры (samples)} \rightarrow \textit{Предсказания (targets)}.
	\end{equation*}
	Данная функция -- \textbf{модель}, а набор примеров -- \textbf{обучающая выборка (dataset)}.
	\begin{equation*}
		\text{Обучающая выборка} = \text{Объекты} + \text{Ответы}.
	\end{equation*}
	Качество таких предсказаний измеряют \textbf{метриками} -- функциями, которые показывают насколько полученные предсказания похожи на правильные ответы. Примером метрики является \textbf{среднее абсолютное отклонение}:
	\begin{equation*}
		MAE(f,X,y) = L(f,X,y) = \frac{1}{N}\sum_{i=1}^{N}|f(x_i) - y_i|. 
	\end{equation*}
	На сегодня достаточно знать два типа моделей -- \textbf{градиентный бустинг на решающий деревьях} и \textbf{нейросетевые модели}.
	
	Метрику, которую используют при поиске оптимальной модели -- \textbf{функция потерь, лосс-функцией (loss)}.
	
	\section{Виды задач}
	
	Определенные выше задачи -- \textbf{обучение с учителем(supervised learning)}, так как правильные ответы были даны заранее. Виды таких обучений: 
	\begin{itemize}
		\item $\mathbb{Y} = \mathbb{R}$ или $\mathbb{Y} = \mathbb{R}^M$ -- \textbf{регрессия};
		\item $\mathbb{Y} = \{0,1\}$ -- \textbf{бинарная классификация};
		\item $\mathbb{Y} = \{1,..,K\}$ -- \textbf{многоклассовая (multiclass) классификация};
		\item $\mathbb{Y} = \{0,1\}^K$ -- \textbf{многоклассовая классификация с пересекающимися классами (multilabel classification)};
		\item $\mathbb{Y}$ -- конечное упорядоченное множество -- \textbf{ранжирование}.
	\end{itemize}
	
	Имеется другой класс задач--\textbf{обучение без учителя (unsupervised learning)}-- для которой известны только данные, а ответы отсутствуют. Одним из примеров является \textit{кластеризация} -- задача разделения объектов на группы, обладающие некоторыми свойствами.
	
	\section{Выбор модели, переобучение}
	
	\textbf{Обобщающаяся способность} модели -- способность модели учить общие закономерности и давать адекватные предсказания. Выборку для этого делят на две части: \textbf{обучающая выборка} и \textbf{тестовая выборка} (\textbf{train} и \textbf{test}). 
	
	Такой подход позволяет отделить модели, которые просто удачно подстроились к обучающим данным, от моделей, в которых произошла \textbf{генерализация (generaliza-tion)}, то есть от таких, которые на самом деле кое-что поняли о том, как устроены данные, и могут выдавать полезные предсказания для объектов, которые не видели.
	
	\textbf{Переобученный алгоритм} -- алгоритм, избыточно подстроившийся под данные.
	
	\chapter{Классическое обучение с учителем}
	
	\section{Линейные модели}
\end{document}